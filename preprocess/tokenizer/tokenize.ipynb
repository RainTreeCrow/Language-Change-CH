{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "stopfile = '../../corpus/stopwords.txt'\n",
    "sentdir = '../../corpus/sent/'\n",
    "yearlydir = '../../corpus/1-year/'\n",
    "frequencydir = '../../corpus/frequency/'\n",
    "\n",
    "with open(stopfile, 'r', encoding='utf-8') as sf:\n",
    "    stopword_list = [word.strip('\\n') for word in sf.readlines()]\n",
    "\n",
    "for newdir in [yearlydir, frequencydir]:\n",
    "    if not os.path.exists(newdir):\n",
    "        os.makedirs(newdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_write(inputdir, outputdir, countdir):\n",
    "    counts = {}\n",
    "    with open(inputdir, 'r', encoding='utf-8') as infile, open(outputdir, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            sentence = line.strip()\n",
    "            if sentence:\n",
    "                words = jieba.lcut(sentence)\n",
    "                temp = \"\"\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    if not (re.search('[^\\.\\-0-9a-zA-Z\\u4e00-\\u9fa5]+', word) or re.match('[\\.\\-\\d]+', word)):\n",
    "                        if word not in stopword_list:\n",
    "                            temp += word + \" \"\n",
    "                            if word in counts.keys():\n",
    "                                counts[word] = counts[word] + 1\n",
    "                            else:\n",
    "                                counts[word] = 1\n",
    "                temp = temp.rstrip()\n",
    "                if temp:\n",
    "                    outfile.write(temp + '\\n')\n",
    "    \n",
    "    counts_list = list(counts.items())\n",
    "    counts_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    with open(countdir, 'w', encoding='utf-8') as countfile:\n",
    "        for (word, freq) in counts_list:\n",
    "            word_freq = word + ' ' + str(freq) + '\\n'\n",
    "            countfile.write(word_freq)\n",
    "\n",
    "\n",
    "def process_all_years(input_folder, output_folder, count_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        year = filename.split('.')[0]\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        output_file = os.path.join(output_folder, f'{year}.txt')\n",
    "        count_file = os.path.join(count_folder, f'{year}.txt')\n",
    "        segment_and_write(input_file, output_file, count_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_years(sentdir, yearlydir, frequencydir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1946...\n",
      "Processing: 1947...\n",
      "Processing: 1948...\n",
      "Processing: 1949...\n",
      "Processing: 1950...\n",
      "Processing: 1951...\n",
      "Processing: 1952...\n",
      "Processing: 1953...\n",
      "Processing: 1954...\n",
      "Processing: 1955...\n",
      "Processing: 1956...\n",
      "Processing: 1957...\n",
      "Processing: 1958...\n",
      "Processing: 1959...\n",
      "Processing: 1960...\n",
      "Processing: 1961...\n",
      "Processing: 1962...\n",
      "Processing: 1963...\n",
      "Processing: 1964...\n",
      "Processing: 1965...\n",
      "Processing: 1966...\n",
      "Processing: 1967...\n",
      "Processing: 1968...\n",
      "Processing: 1969...\n",
      "Processing: 1970...\n",
      "Processing: 1971...\n",
      "Processing: 1972...\n",
      "Processing: 1973...\n",
      "Processing: 1974...\n",
      "Processing: 1975...\n",
      "Processing: 1976...\n",
      "Processing: 1977...\n",
      "Processing: 1978...\n",
      "Processing: 1979...\n",
      "Processing: 1980...\n",
      "Processing: 1981...\n",
      "Processing: 1982...\n",
      "Processing: 1983...\n",
      "Processing: 1984...\n",
      "Processing: 1985...\n",
      "Processing: 1986...\n",
      "Processing: 1987...\n",
      "Processing: 1988...\n",
      "Processing: 1989...\n",
      "Processing: 1990...\n",
      "Processing: 1991...\n",
      "Processing: 1992...\n",
      "Processing: 1993...\n",
      "Processing: 1994...\n",
      "Processing: 1995...\n",
      "Processing: 1996...\n",
      "Processing: 1997...\n",
      "Processing: 1998...\n",
      "Processing: 1999...\n",
      "Processing: 2000...\n",
      "Processing: 2001...\n",
      "Processing: 2002...\n",
      "Processing: 2003...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1946, 2024):\n",
    "\n",
    "    repr = str(i)\n",
    "    # print(\"Processing: \" + repr + \"...\")\n",
    "\n",
    "    rootdir = rawdir + repr + '年'\n",
    "    newfile = yearlydir + repr + '.txt'\n",
    "    countfile = frequencydir + repr + '.txt'\n",
    "\n",
    "    paths = []\n",
    "    counts = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            paths.append(os.path.join(root, file).encode('utf-8'))\n",
    "\n",
    "    f = open(newfile,'wb')\n",
    "    cf = open(countfile, 'wb')\n",
    "\n",
    "    for i in paths:\n",
    "        \n",
    "        fileTrainRead = []\n",
    "        with open(i, encoding='utf-8') as fileTrainRaw:\n",
    "            for line in fileTrainRaw:\n",
    "                fileTrainRead.append(re.sub(r\"第.*版\\(.*\\)专栏：\", \"\", line))\n",
    "\n",
    "        # Each line a piece of news\n",
    "        fileTrainSeg = []\n",
    "        for news in fileTrainRead:\n",
    "            temp = \"\"\n",
    "            for word in jieba.lcut(news):\n",
    "                if not (re.search('[^\\.\\-0-9a-zA-Z\\u4e00-\\u9fa5]+', word) or re.match('[\\.\\-\\d]+', word)):\n",
    "                    if word not in stopword_list:\n",
    "                        temp += word + \" \"\n",
    "                        if word in counts.keys():\n",
    "                            counts[word] = counts[word] + 1\n",
    "                        else:\n",
    "                            counts[word] = 1\n",
    "            temp = temp.rstrip()\n",
    "            if temp:\n",
    "                fileTrainSeg.append(temp)\n",
    "\n",
    "        for i in range(len(fileTrainSeg)):\n",
    "            f.write(fileTrainSeg[i].encode('utf-8'))\n",
    "            f.write(\"\\n\".encode('utf-8'))\n",
    "    \n",
    "    counts_list = list(counts.items())\n",
    "    counts_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    for (word, freq) in counts_list:\n",
    "        word_freq = word + ' ' + str(freq) + '\\n'\n",
    "        cf.write(word_freq.encode('utf-8'))\n",
    "\n",
    "    f.close()\n",
    "    cf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
