{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News before 2003 are crawled from \"老资料网\" (https://www.laoziliao.net/rmrb), where each line consists of one full news article starting with \"第N版()专栏：\", which I removed during preprocessing. News after 2004 are crawled from \"人民日报图文数据库\" (http://paper.people.com.cn/rmrb), each file consists of an article without the column, so they are treated seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pkuseg\n",
    "\n",
    "stopfile = 'D:/人民日报/stopwords.txt'\n",
    "rawdir = 'D:/人民日报/raw/'\n",
    "yearlydir = 'D:/人民日报/1-year/'\n",
    "frequencydir = 'D:/人民日报/frequency/'\n",
    "\n",
    "with open(stopfile, 'r', encoding='utf-8') as sf:\n",
    "    stopword_list = [word.strip('\\n') for word in sf.readlines()]\n",
    "\n",
    "seg = pkuseg.pkuseg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1946, 2004):\n",
    "\n",
    "    repr = str(i)\n",
    "    print(\"Processing: \" + repr + \"...\")\n",
    "\n",
    "    rootdir = rawdir + repr + '年'\n",
    "    newfile = yearlydir + repr + '.txt'\n",
    "    countfile = frequencydir + repr + '.txt'\n",
    "\n",
    "    paths = []\n",
    "    counts = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            paths.append(os.path.join(root, file).encode('utf-8'))\n",
    "\n",
    "    f = open(newfile,'wb')\n",
    "    cf = open(countfile, 'wb')\n",
    "\n",
    "    for i in paths:\n",
    "        \n",
    "        fileTrainRead = []\n",
    "        with open(i, encoding='utf-8') as fileTrainRaw:\n",
    "            for line in fileTrainRaw:\n",
    "                fileTrainRead.append(re.sub(r\"第.*版\\(.*\\)专栏：\", \"\", line))\n",
    "\n",
    "        # Each line a piece of news\n",
    "        fileTrainSeg = []\n",
    "        for news in fileTrainRead:\n",
    "            temp = \"\"\n",
    "            for word in seg.cut(news):\n",
    "                if not (re.search('[^\\.\\-0-9a-zA-Z\\u4e00-\\u9fa5]+', word) or re.match('[\\.\\-\\d]+', word)):\n",
    "                    if word not in stopword_list:\n",
    "                        temp += word + \" \"\n",
    "                        if word in counts.keys():\n",
    "                            counts[word] = counts[word] + 1\n",
    "                        else:\n",
    "                            counts[word] = 1\n",
    "            temp = temp.rstrip()\n",
    "            if temp:\n",
    "                fileTrainSeg.append(temp)\n",
    "\n",
    "        for i in range(len(fileTrainSeg)):\n",
    "            f.write(fileTrainSeg[i].encode('utf-8'))\n",
    "            f.write(\"\\n\".encode('utf-8'))\n",
    "    \n",
    "    counts_list = list(counts.items())\n",
    "    counts_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    for (word, freq) in counts_list:\n",
    "        word_freq = word + ' ' + str(freq) + '\\n'\n",
    "        cf.write(word_freq.encode('utf-8'))\n",
    "\n",
    "    f.close()\n",
    "    cf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2004, 2024):\n",
    "    \n",
    "    repr = str(i)\n",
    "    print(\"Processing: \" + repr + \"...\")\n",
    "\n",
    "    rootdir = rawdir + repr + '年'\n",
    "    newfile = yearlydir + repr + '.txt'\n",
    "    countfile = frequencydir + repr + '.txt'\n",
    "    \n",
    "    paths = []\n",
    "    counts = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            paths.append(os.path.join(root, file).encode('utf-8'))\n",
    "\n",
    "    f = open(newfile,'wb')\n",
    "    cf = open(countfile, 'wb')\n",
    "    \n",
    "    for i in paths:\n",
    "\n",
    "        fileTrainRead = []\n",
    "        with open(i, encoding='utf-8') as fileTrainRaw:\n",
    "            for line in fileTrainRaw:\n",
    "                fileTrainRead.append(line)\n",
    "\n",
    "        # Each file a piece of news\n",
    "        fileTrainSeg = \"\"\n",
    "        for sentence in fileTrainRead:\n",
    "            temp = \"\"\n",
    "            for word in seg.cut(sentence):\n",
    "                if not (re.search('[^\\.\\-0-9a-zA-Z\\u4e00-\\u9fa5]+', word) or re.match('[\\.\\-\\d]+', word)):\n",
    "                    if word not in stopword_list:\n",
    "                        temp += word + \" \"\n",
    "                        if word in counts.keys():\n",
    "                            counts[word] = counts[word] + 1\n",
    "                        else:\n",
    "                            counts[word] = 1\n",
    "            if temp:\n",
    "                fileTrainSeg += temp\n",
    "        \n",
    "        fileTrainSeg = fileTrainSeg.rstrip()\n",
    "        f.write(fileTrainSeg.encode('utf-8'))\n",
    "        f.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "    counts_list = list(counts.items())\n",
    "    counts_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    for (word, freq) in counts_list:\n",
    "        word_freq = word + ' ' + str(freq) + '\\n'\n",
    "        cf.write(word_freq.encode('utf-8'))\n",
    "\n",
    "    f.close()\n",
    "    cf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
