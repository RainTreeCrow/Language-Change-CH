{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News before 2003 are crawled from \"老资料网\" (https://www.laoziliao.net/rmrb), where each line consists of one full news article starting with \"第N版()专栏：\", which I removed during preprocessing. News after 2004 are crawled from \"人民日报图文数据库\" (http://paper.people.com.cn/rmrb), each file consists of an article without the column, so they are treated seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "stopfile = '../corpus/stopwords.txt'\n",
    "rawdir = '../corpus/raw/'\n",
    "yearlydir = '../corpus/1-year/'\n",
    "frequencydir = '../corpus/frequency/'\n",
    "\n",
    "with open(stopfile, 'r', encoding='utf-8') as sf:\n",
    "    stopword_list = [word.strip('\\n') for word in sf.readlines()]\n",
    "\n",
    "for newdir in [yearlydir, frequencydir]:\n",
    "    if not os.path.exists(newdir):\n",
    "        os.makedirs(newdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1946...\n",
      "Processing: 1947...\n",
      "Processing: 1948...\n",
      "Processing: 1949...\n",
      "Processing: 1950...\n",
      "Processing: 1951...\n",
      "Processing: 1952...\n",
      "Processing: 1953...\n",
      "Processing: 1954...\n",
      "Processing: 1955...\n",
      "Processing: 1956...\n",
      "Processing: 1957...\n",
      "Processing: 1958...\n",
      "Processing: 1959...\n",
      "Processing: 1960...\n",
      "Processing: 1961...\n",
      "Processing: 1962...\n",
      "Processing: 1963...\n",
      "Processing: 1964...\n",
      "Processing: 1965...\n",
      "Processing: 1966...\n",
      "Processing: 1967...\n",
      "Processing: 1968...\n",
      "Processing: 1969...\n",
      "Processing: 1970...\n",
      "Processing: 1971...\n",
      "Processing: 1972...\n",
      "Processing: 1973...\n",
      "Processing: 1974...\n",
      "Processing: 1975...\n",
      "Processing: 1976...\n",
      "Processing: 1977...\n",
      "Processing: 1978...\n",
      "Processing: 1979...\n",
      "Processing: 1980...\n",
      "Processing: 1981...\n",
      "Processing: 1982...\n",
      "Processing: 1983...\n",
      "Processing: 1984...\n",
      "Processing: 1985...\n",
      "Processing: 1986...\n",
      "Processing: 1987...\n",
      "Processing: 1988...\n",
      "Processing: 1989...\n",
      "Processing: 1990...\n",
      "Processing: 1991...\n",
      "Processing: 1992...\n",
      "Processing: 1993...\n",
      "Processing: 1994...\n",
      "Processing: 1995...\n",
      "Processing: 1996...\n",
      "Processing: 1997...\n",
      "Processing: 1998...\n",
      "Processing: 1999...\n",
      "Processing: 2000...\n",
      "Processing: 2001...\n",
      "Processing: 2002...\n",
      "Processing: 2003...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1946, 2004):\n",
    "\n",
    "    repr = str(i)\n",
    "    print(\"Processing: \" + repr + \"...\")\n",
    "\n",
    "    rootdir = rawdir + repr + '年'\n",
    "    newfile = yearlydir + repr + '.txt'\n",
    "    countfile = frequencydir + repr + '.txt'\n",
    "\n",
    "    paths = []\n",
    "    counts = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            paths.append(os.path.join(root, file).encode('utf-8'))\n",
    "\n",
    "    f = open(newfile,'wb')\n",
    "    cf = open(countfile, 'wb')\n",
    "\n",
    "    for i in paths:\n",
    "        \n",
    "        fileTrainRead = []\n",
    "        with open(i, encoding='utf-8') as fileTrainRaw:\n",
    "            for line in fileTrainRaw:\n",
    "                fileTrainRead.append(re.sub(r\"第.*版\\(.*\\)专栏：\", \"\", line))\n",
    "\n",
    "        # Each line a piece of news\n",
    "        fileTrainSeg = []\n",
    "        for news in fileTrainRead:\n",
    "            temp = \"\"\n",
    "            for word in jieba.lcut(news):\n",
    "                if not (re.search('[^\\.\\-0-9a-zA-Z\\u4e00-\\u9fa5]+', word) or re.match('[\\.\\-\\d]+', word)):\n",
    "                    if word not in stopword_list:\n",
    "                        temp += word + \" \"\n",
    "                        if word in counts.keys():\n",
    "                            counts[word] = counts[word] + 1\n",
    "                        else:\n",
    "                            counts[word] = 1\n",
    "            temp = temp.rstrip()\n",
    "            if temp:\n",
    "                fileTrainSeg.append(temp)\n",
    "\n",
    "        for i in range(len(fileTrainSeg)):\n",
    "            f.write(fileTrainSeg[i].encode('utf-8'))\n",
    "            f.write(\"\\n\".encode('utf-8'))\n",
    "    \n",
    "    counts_list = list(counts.items())\n",
    "    counts_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    for (word, freq) in counts_list:\n",
    "        word_freq = word + ' ' + str(freq) + '\\n'\n",
    "        cf.write(word_freq.encode('utf-8'))\n",
    "\n",
    "    f.close()\n",
    "    cf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2004...\n",
      "Processing: 2005...\n",
      "Processing: 2006...\n",
      "Processing: 2007...\n",
      "Processing: 2008...\n",
      "Processing: 2009...\n",
      "Processing: 2010...\n",
      "Processing: 2011...\n",
      "Processing: 2012...\n",
      "Processing: 2013...\n",
      "Processing: 2014...\n",
      "Processing: 2015...\n",
      "Processing: 2016...\n",
      "Processing: 2017...\n",
      "Processing: 2018...\n",
      "Processing: 2019...\n",
      "Processing: 2020...\n",
      "Processing: 2021...\n",
      "Processing: 2022...\n",
      "Processing: 2023...\n"
     ]
    }
   ],
   "source": [
    "for i in range(2004, 2024):\n",
    "    \n",
    "    repr = str(i)\n",
    "    print(\"Processing: \" + repr + \"...\")\n",
    "\n",
    "    rootdir = rawdir + repr + '年'\n",
    "    newfile = yearlydir + repr + '.txt'\n",
    "    countfile = frequencydir + repr + '.txt'\n",
    "    \n",
    "    paths = []\n",
    "    counts = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            paths.append(os.path.join(root, file).encode('utf-8'))\n",
    "\n",
    "    f = open(newfile,'wb')\n",
    "    cf = open(countfile, 'wb')\n",
    "    \n",
    "    for i in paths:\n",
    "\n",
    "        fileTrainRead = []\n",
    "        with open(i, encoding='utf-8') as fileTrainRaw:\n",
    "            for line in fileTrainRaw:\n",
    "                fileTrainRead.append(line)\n",
    "\n",
    "        # Each file a piece of news\n",
    "        fileTrainSeg = \"\"\n",
    "        for sentence in fileTrainRead:\n",
    "            temp = \"\"\n",
    "            for word in jieba.lcut(sentence):\n",
    "                if not (re.search('[^\\.\\-0-9a-zA-Z\\u4e00-\\u9fa5]+', word) or re.match('[\\.\\-\\d]+', word)):\n",
    "                    if word not in stopword_list:\n",
    "                        temp += word + \" \"\n",
    "                        if word in counts.keys():\n",
    "                            counts[word] = counts[word] + 1\n",
    "                        else:\n",
    "                            counts[word] = 1\n",
    "            if temp:\n",
    "                fileTrainSeg += temp\n",
    "        \n",
    "        fileTrainSeg = fileTrainSeg.rstrip()\n",
    "        f.write(fileTrainSeg.encode('utf-8'))\n",
    "        f.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "    counts_list = list(counts.items())\n",
    "    counts_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    for (word, freq) in counts_list:\n",
    "        word_freq = word + ' ' + str(freq) + '\\n'\n",
    "        cf.write(word_freq.encode('utf-8'))\n",
    "\n",
    "    f.close()\n",
    "    cf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
