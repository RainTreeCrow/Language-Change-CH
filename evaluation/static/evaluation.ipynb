{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing quality of static (compass) word embeddings (similarity task and analogy task) and temporal (target) embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = 'static_eval.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['model', 'type', 'slice', '240', '297',\n",
    "    'capital_mrr', 'capital_mp1', 'capital_mp5', 'capital_mp10',\n",
    "    'city_mrr', 'city_mp1', 'city_mp5', 'city_mp10',\n",
    "    'family_mrr', 'family_mp1', 'family_mp5', 'family_mp10',\n",
    "    'total_mrr', 'total_mp1', 'total_mp5', 'total_mp10']\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "def cosine_sim(x,y):\n",
    "    num = np.dot(x, y)\n",
    "    denom = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "    return num / denom\n",
    "\n",
    "\n",
    "def word_sim(model):\n",
    "    sim_scores = []\n",
    "\n",
    "    for sim in ['240', '297']:\n",
    "        file = open('word_sim/' + sim + '.txt', 'r', encoding='utf-8')\n",
    "        test_pair_num = 0\n",
    "        skip_pair_num = 0\n",
    "        word_sim_std = []\n",
    "        word_sim_pre = []\n",
    "        for line in file:\n",
    "            word1, word2, val_str = line.strip().split()[0:3]\n",
    "            if word1 in model.wv and word2 in model.wv:\n",
    "                test_pair_num += 1\n",
    "                word_sim_std.append(float(val_str))\n",
    "                word_vec1 = model.wv[word1]\n",
    "                word_vec2 = model.wv[word2]\n",
    "                cos_sim = cosine_sim(word_vec1, word_vec2)\n",
    "                word_sim_pre.append(cos_sim)\n",
    "            else:\n",
    "                skip_pair_num += 1\n",
    "                # print('Skip:', word1, word2)\n",
    "\n",
    "        spear_coef, _ = stats.spearmanr(word_sim_std, word_sim_pre)\n",
    "        sim_scores.append(spear_coef)\n",
    "\n",
    "    return sim_scores\n",
    "\n",
    "\n",
    "def reciprocal_rank(neighbours, target_word):\n",
    "    for rank, word in enumerate(neighbours, start=1):\n",
    "        if word == target_word:\n",
    "            return 1.0 / rank\n",
    "    # If target word not in 'neighbours', return 0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def precision(neighbours, target_word, k):\n",
    "    if target_word in neighbours[:k]:\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def word_analogy(model):\n",
    "    reciprocal_all = 0\n",
    "    precision_1_all = 0\n",
    "    precision_5_all = 0\n",
    "    precision_10_all = 0\n",
    "    total_all = 0\n",
    "    analogy_scores = []\n",
    "\n",
    "    for ana in ['capital', 'city', 'family']:\n",
    "        file = open('word_analogy/' + ana + '.txt', 'r', encoding='utf-8')\n",
    "        test_pair_num = 0\n",
    "        reciprocal = 0\n",
    "        precision_1 = 0\n",
    "        precision_5 = 0\n",
    "        precision_10 = 0\n",
    "        for line in file:\n",
    "            word1, word2, word3, word4 = line.strip().split()[0:4]\n",
    "            if word1 in model.wv and word2 in model.wv and word3 in model.wv:\n",
    "                test_pair_num += 1\n",
    "                predicted = model.wv.most_similar(positive=[word2, word3], negative=[word1])\n",
    "                neighbours = [pair[0] for pair in predicted]\n",
    "                reciprocal += reciprocal_rank(neighbours, word4)\n",
    "                precision_1 += precision(neighbours, word4, 1)\n",
    "                precision_5 += precision(neighbours, word4, 5)\n",
    "                precision_10 += precision(neighbours, word4, 10)\n",
    "        \n",
    "        reciprocal_all += reciprocal\n",
    "        precision_1_all += precision_1\n",
    "        precision_5_all += precision_5\n",
    "        precision_10_all += precision_10\n",
    "        total_all += test_pair_num\n",
    "        analogy_scores.extend([reciprocal / test_pair_num, \n",
    "            precision_1 / test_pair_num,\n",
    "            precision_5 / test_pair_num,\n",
    "            precision_10 / test_pair_num])\n",
    "    \n",
    "    analogy_scores.extend([reciprocal_all / total_all,\n",
    "        precision_1_all / total_all,\n",
    "        precision_5_all / total_all,\n",
    "        precision_10_all / total_all])\n",
    "    \n",
    "    return analogy_scores\n",
    "\n",
    "\n",
    "def static_eval(model, m_name, m_type, m_slice):\n",
    "    row = [m_name, m_type, m_slice]\n",
    "    row.extend(word_sim(model))\n",
    "    row.extend(word_analogy(model))\n",
    "    \n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_graph(slices, folder, ticks, labels):\n",
    "    df = pd.read_csv('static_eval.csv')\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    combinations = df[['model', 'type']].drop_duplicates()\n",
    "\n",
    "    for index, row in combinations.iterrows():\n",
    "        model_name = row['model']\n",
    "        model_type = row['type']\n",
    "        subset = df[(df['model'] == model_name) & (df['type'] == model_type) & (df['slice'].isin(slices))]\n",
    "        subset = subset.sort_values(by='slice')\n",
    "\n",
    "        axes[0].plot(subset['slice'], subset['240'], label=f'{model_name}-{model_type}')\n",
    "        axes[0].set_title('Word Similarity 240')\n",
    "        axes[0].set_xlabel('year slice')\n",
    "        axes[0].set_ylabel('Spearman Correlation')\n",
    "        \n",
    "        axes[1].plot(subset['slice'], subset['297'], label=f'{model_name}-{model_type}')\n",
    "        axes[1].set_title('Word Similarity 297')\n",
    "        axes[1].set_xlabel('year slice')\n",
    "        axes[1].set_ylabel('Spearman Correlation')\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xticks(ticks)\n",
    "    axes[0].set_xticklabels(labels, rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xticks(ticks)\n",
    "    axes[1].set_xticklabels(labels, rotation=45)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(folder + 'word_sim.png')\n",
    "    plt.close()\n",
    "\n",
    "    for analogy in ['capital', 'city', 'family', 'total']:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "        for index, row in combinations.iterrows():\n",
    "            model_name = row['model']\n",
    "            model_type = row['type']\n",
    "            subset = df[(df['model'] == model_name) & (df['type'] == model_type) & (df['slice'].isin(slices))]\n",
    "            subset = subset.sort_values(by='slice')\n",
    "\n",
    "            axes[0][0].plot(subset['slice'], subset[analogy + '_mrr'], label=f'{model_name}-{model_type}')\n",
    "            axes[0][0].set_title('Analogy ' + analogy.capitalize() + ' MRR')\n",
    "            axes[0][0].set_xlabel('year slice')\n",
    "            axes[0][0].set_ylabel('Mean Reciprocal Rank')\n",
    "            \n",
    "            axes[0][1].plot(subset['slice'], subset[analogy + '_mp1'], label=f'{model_name}-{model_type}')\n",
    "            axes[0][1].set_title('Analogy ' + analogy.capitalize() + ' MP@1')\n",
    "            axes[0][1].set_xlabel('year slice')\n",
    "            axes[0][1].set_ylabel('Mean Precision @ 1')\n",
    "\n",
    "            axes[1][0].plot(subset['slice'], subset[analogy + '_mp5'], label=f'{model_name}-{model_type}')\n",
    "            axes[1][0].set_title('Analogy ' + analogy.capitalize() + ' MP@5')\n",
    "            axes[1][0].set_xlabel('year slice')\n",
    "            axes[1][0].set_ylabel('Mean Precision @ 5')\n",
    "\n",
    "            axes[1][1].plot(subset['slice'], subset[analogy + '_mp10'], label=f'{model_name}-{model_type}')\n",
    "            axes[1][1].set_title('Analogy ' + analogy.capitalize() + ' MP@10')\n",
    "            axes[1][1].set_xlabel('year slice')\n",
    "            axes[1][1].set_ylabel('Mean Precision @ 10')\n",
    "        \n",
    "        axes[0][0].legend()\n",
    "        axes[0][0].set_xticks(ticks)\n",
    "        axes[0][0].set_xticklabels(labels, rotation=45)\n",
    "        axes[0][1].legend()\n",
    "        axes[0][1].set_xticks(ticks)\n",
    "        axes[0][1].set_xticklabels(labels, rotation=45)\n",
    "        axes[1][0].legend()\n",
    "        axes[1][0].set_xticks(ticks)\n",
    "        axes[1][0].set_xticklabels(labels, rotation=45)\n",
    "        axes[1][1].legend()\n",
    "        axes[1][1].set_xticks(ticks)\n",
    "        axes[1][1].set_xticklabels(labels, rotation=45)\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(folder + analogy + '.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the quality of 2-slices embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year_slice in ['1954-1978', '1979-2003']:\n",
    "    for model_name in ['compass', 'alignment']:\n",
    "        for model_type in ['sgns', 'cbow']:\n",
    "            model_path = '../../' + model_name + '/2-slices/model-' + model_type + '/' + year_slice + '.model'\n",
    "            model = Word2Vec.load(model_path)\n",
    "            static_eval(model, model_name, model_type, year_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_slices = ['1954-1978', '1979-2003']\n",
    "labels = ['before', 'after']\n",
    "ticks = [0, 1]\n",
    "draw_graph(two_slices, 'graphic/2-slices/', ticks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the quality of 5-year and 1-year slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1945, 2025, 5):\n",
    "    year_slice = str(i) + '-' + str(i + 4)\n",
    "    for model_name in ['compass', 'alignment']:\n",
    "        for model_type in ['sgns', 'cbow']:\n",
    "            model_path = '../../' + model_name + '/5-year/model-' + model_type + '/' + year_slice + '.model'\n",
    "            model = Word2Vec.load(model_path)\n",
    "            static_eval(model, model_name, model_type, year_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year = [str(i) + '-' + str(i + 4) for i in range(1945, 2025, 5)]\n",
    "labels = [str(i) for i in range(1945, 2025, 5)]\n",
    "ticks = range(len(labels))\n",
    "draw_graph(five_year, 'graphic/5-year/', ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1946, 2024):\n",
    "    year_slice = str(i)\n",
    "    for model_name in ['compass', 'alignment']:\n",
    "        for model_type in ['sgns', 'cbow']:\n",
    "            model_path = '../../' + model_name + '/1-year/model-' + model_type + '/' + year_slice + '.model'\n",
    "            model = Word2Vec.load(model_path)\n",
    "            static_eval(model, model_name, model_type, year_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year = [str(i) for i in range(1946, 2024)]\n",
    "labels = [str(i) for i in range(1945, 2030, 5)]\n",
    "ticks = [i * 5 for i in range(len(labels))]\n",
    "draw_graph(one_year, 'graphic/1-year/', ticks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between wordsim test scores, year and total word count each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def is_numeric(value):\n",
    "    return bool(re.fullmatch(r'\\d+', str(value)))\n",
    "\n",
    "raw_stats = pd.read_csv('../../preprocess/raw_stats/raw_stats.csv')\n",
    "stat_eval = pd.read_csv('static_eval.csv')\n",
    "stat_eval = stat_eval[stat_eval['slice'].apply(is_numeric)]\n",
    "stat_eval['year'] = stat_eval['slice'].astype(int)\n",
    "\n",
    "merged_data = pd.merge(stat_eval, raw_stats, left_on='year', right_on='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman correlation between year and word count is 0.7548274510299826\n"
     ]
    }
   ],
   "source": [
    "corr_year_length, _ = spearmanr(raw_stats['year'], raw_stats['total_length'])\n",
    "print('The Spearman correlation between year and word count is', corr_year_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results = []\n",
    "\n",
    "for model in merged_data['model'].unique():\n",
    "    for type_ in merged_data['type'].unique():\n",
    "        subset = merged_data[(merged_data['model'] == model) & (merged_data['type'] == type_)]\n",
    "        \n",
    "        corr_240_year, _ = spearmanr(subset['240'], subset['year'])\n",
    "        corr_297_year, _ = spearmanr(subset['297'], subset['year'])\n",
    "        corr_240_length, _ = spearmanr(subset['240'], subset['total_length'])\n",
    "        corr_297_length, _ = spearmanr(subset['297'], subset['total_length'])\n",
    "        \n",
    "        correlation_results.append({\n",
    "            'model': model,\n",
    "            'type': type_,\n",
    "            '240_vs_year': corr_240_year,\n",
    "            '297_vs_year': corr_297_year,\n",
    "            '240_vs_wordcount': corr_240_length,\n",
    "            '297_vs_wordcount': corr_297_length\n",
    "        })\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_results)\n",
    "correlation_df.to_csv('correlation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
