{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = 'temp_analogy.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = ['ch_leader', 'ch_premier', 'us_president', 'us_secretary', 'uk_minister',\n",
    "               'fr_president', 'de_chancellor', 'jp_minister', 'kr_president']\n",
    "\n",
    "fieldnames = ['model', 'type', 'depth']\n",
    "\n",
    "for politician in politicians:\n",
    "    fieldnames.extend([\n",
    "        f'{politician}_mrr', f'{politician}_mp1', f'{politician}_mp5', f'{politician}_mp10'\n",
    "    ])\n",
    "\n",
    "fieldnames.extend(['total_mrr', 'total_mp1', 'total_mp5', 'total_mp10'])\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "def reciprocal_rank(neighbours, target_words):\n",
    "    for rank, word in enumerate(neighbours, start=1):\n",
    "        if word in target_words:\n",
    "            return 1.0 / rank\n",
    "    # If target word not in 'neighbours', return 0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def precision(neighbours, target_words, k):\n",
    "    for target_word in target_words:\n",
    "        for neighbour in neighbours[:k]:\n",
    "            if neighbour in target_word:\n",
    "                return 1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def temporal_analogy(models):\n",
    "    analogy_scores_list = []\n",
    "\n",
    "    df = pd.read_csv('facts/politicians.csv')\n",
    "    columns = politicians\n",
    "\n",
    "    for depth in range (5, 85, 5):\n",
    "        reciprocal_all = 0\n",
    "        precision_1_all = 0\n",
    "        precision_5_all = 0\n",
    "        precision_10_all = 0\n",
    "        total_all = 0\n",
    "        analogy_scores = [depth]\n",
    "        for column in columns:\n",
    "            test_pair_num = 0\n",
    "            reciprocal = 0\n",
    "            precision_1 = 0\n",
    "            precision_5 = 0\n",
    "            precision_10 = 0\n",
    "            non_empty_rows = df[df[column].notna()][['year', column]]\n",
    "            \n",
    "            for index, row in non_empty_rows.iterrows():\n",
    "                search_year = row['year']\n",
    "                column_value = row[column]\n",
    "                split_values = column_value.split('/')\n",
    "                if len(split_values) == 1:\n",
    "                    search_word = split_values[0]\n",
    "                    if search_word in models[search_year-1946].wv:\n",
    "                        search_vector = models[search_year-1946].wv[search_word]\n",
    "                        other_rows = non_empty_rows[(non_empty_rows.index != index) & \n",
    "                            (abs(non_empty_rows['year'] - search_year) <= depth) &\n",
    "                            (abs(non_empty_rows['year'] - search_year) > depth - 5)]\n",
    "                        \n",
    "                        if not other_rows.empty:\n",
    "                            for _, other_row in other_rows.iterrows():\n",
    "                                target_year = other_row['year']\n",
    "                                test_pair_num += 1\n",
    "                                other_column_value = other_row[column]\n",
    "                                target_words = other_column_value.split('/')\n",
    "                                predicted = models[target_year-1946].wv.most_similar([search_vector])\n",
    "                                neighbours = [pair[0] for pair in predicted]\n",
    "                                reciprocal += reciprocal_rank(neighbours, target_words)\n",
    "                                precision_1 += precision(neighbours, target_words, 1)\n",
    "                                precision_5 += precision(neighbours, target_words, 5)\n",
    "                                precision_10 += precision(neighbours, target_words, 10)\n",
    "            \n",
    "            if test_pair_num != 0:\n",
    "                reciprocal_all += reciprocal\n",
    "                precision_1_all += precision_1\n",
    "                precision_5_all += precision_5\n",
    "                precision_10_all += precision_10\n",
    "                total_all += test_pair_num\n",
    "                analogy_scores.extend([reciprocal / test_pair_num, \n",
    "                    precision_1 / test_pair_num,\n",
    "                    precision_5 / test_pair_num,\n",
    "                    precision_10 / test_pair_num])\n",
    "            else:\n",
    "                analogy_scores.extend([None, None, None, None])\n",
    "\n",
    "        if total_all != 0:\n",
    "            analogy_scores.extend([reciprocal_all / total_all,\n",
    "                precision_1_all / total_all,\n",
    "                precision_5_all / total_all,\n",
    "                precision_10_all / total_all])\n",
    "        else:\n",
    "            analogy_scores.extend([None, None, None, None])\n",
    "\n",
    "        analogy_scores_list.append(analogy_scores)\n",
    "    \n",
    "    return analogy_scores_list\n",
    "\n",
    "\n",
    "def temp_ana_eval(models, m_name, m_type):\n",
    "    for scores in temporal_analogy(models):\n",
    "        row = [m_name, m_type]\n",
    "        row.extend(scores)\n",
    "        \n",
    "        with open(csv_file, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_graph():\n",
    "    df = pd.read_csv('temp_analogy.csv')\n",
    "\n",
    "    combinations = df[['model', 'type']].drop_duplicates()\n",
    "    columns = politicians + ['total']\n",
    "\n",
    "    for column in columns:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "        for index, row in combinations.iterrows():\n",
    "            model_name = row['model']\n",
    "            model_type = row['type']\n",
    "            subset = df[(df['model'] == model_name) & (df['type'] == model_type)]\n",
    "            subset = subset.sort_values(by='depth')\n",
    "\n",
    "            axes[0][0].plot(subset['depth'], subset[column + '_mrr'], label=f'{model_name}-{model_type}')\n",
    "            axes[0][0].set_title('Analogy ' + column + ' MRR')\n",
    "            axes[0][0].set_xlabel('depth')\n",
    "            axes[0][0].set_ylabel('Mean Reciprocal Rank')\n",
    "            \n",
    "            axes[0][1].plot(subset['depth'], subset[column + '_mp1'], label=f'{model_name}-{model_type}')\n",
    "            axes[0][1].set_title('Analogy ' + column + ' MP@1')\n",
    "            axes[0][1].set_xlabel('depth')\n",
    "            axes[0][1].set_ylabel('Mean Precision @ 1')\n",
    "\n",
    "            axes[1][0].plot(subset['depth'], subset[column + '_mp5'], label=f'{model_name}-{model_type}')\n",
    "            axes[1][0].set_title('Analogy ' + column + ' MP@5')\n",
    "            axes[1][0].set_xlabel('depth')\n",
    "            axes[1][0].set_ylabel('Mean Precision @ 5')\n",
    "\n",
    "            axes[1][1].plot(subset['depth'], subset[column + '_mp10'], label=f'{model_name}-{model_type}')\n",
    "            axes[1][1].set_title('Analogy ' + column + ' MP@10')\n",
    "            axes[1][1].set_xlabel('depth')\n",
    "            axes[1][1].set_ylabel('Mean Precision @ 10')\n",
    "        \n",
    "        axes[0][0].legend()\n",
    "        axes[0][1].legend()\n",
    "        axes[1][0].legend()\n",
    "        axes[1][1].legend()\n",
    "        fig.tight_layout()\n",
    "        plt.savefig('graphic/' + column + '.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '../../compass/1-year/model-sgns/'\n",
    "models_sgns = []\n",
    "\n",
    "for i in range(1946, 2024):\n",
    "    year = modeldir + str(i) + '.model'\n",
    "    model = Word2Vec.load(year)\n",
    "    models_sgns.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ana_eval(models_sgns, 'compass', 'sgns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '../../compass/1-year/model-cbow/'\n",
    "models_cbow = []\n",
    "\n",
    "for i in range(1946, 2024):\n",
    "    year = modeldir + str(i) + '.model'\n",
    "    model = Word2Vec.load(year)\n",
    "    models_cbow.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ana_eval(models_cbow, 'compass', 'cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw all politicians on the same graph for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_compare():\n",
    "    df = pd.read_csv('temp_analogy.csv')\n",
    "\n",
    "    # Filter data for the specified model and type\n",
    "    df_filtered = df[(df['model'] == 'compass') & (df['type'] == 'cbow')]\n",
    "\n",
    "    # Define the columns to plot (politicians)\n",
    "    columns = ['ch_leader', 'ch_premier', 'us_president', 'us_secretary', 'uk_minister',\n",
    "               'fr_president', 'de_chancellor', 'jp_minister', 'kr_president']\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "    # Iterate over metrics to create subplots\n",
    "    metrics = ['mrr', 'mp1', 'mp5', 'mp10']\n",
    "    titles = ['Analogy MRR', 'Analogy MP@1', 'Analogy MP@5', 'Analogy MP@10']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 2][i % 2]\n",
    "        for col in columns:\n",
    "            subset = df_filtered.sort_values(by='depth')\n",
    "            ax.plot(subset['depth'], subset[f'{col}_{metric}'], label=col)\n",
    "        \n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel('depth')\n",
    "        ax.set_ylabel('Mean Reciprocal Rank' if metric == 'mrr' else f'Mean Precision @ {metric[-1]}')\n",
    "        ax.legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('compare.png')\n",
    "    plt.close()\n",
    "\n",
    "# Run the function\n",
    "draw_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the frequency of politician names each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths to the files and folders\n",
    "politicians_file = 'facts/politicians.csv'\n",
    "frequency_folder = '../../corpus/frequency'\n",
    "output_file = 'politician_freq.csv'\n",
    "\n",
    "# Read the politicians CSV file\n",
    "politicians_df = pd.read_csv(politicians_file, encoding='utf-8')\n",
    "\n",
    "# Get a list of all years from the politicians CSV file\n",
    "years = politicians_df['year'].tolist()\n",
    "\n",
    "columns = ['ch_leader', 'ch_premier', 'us_president', 'us_secretary', 'uk_minister',\n",
    "           'fr_president', 'de_chancellor', 'jp_minister', 'kr_president']\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "result = {'year': years}\n",
    "\n",
    "# Add the other columns\n",
    "for col in columns:\n",
    "    result[col] = []\n",
    "\n",
    "# Iterate over each year\n",
    "for year in years:\n",
    "    # Path to the frequency file for the current year\n",
    "    frequency_file = os.path.join(frequency_folder, f'{year}.txt')\n",
    "    # Initialize a dictionary to store frequencies for each politician\n",
    "    year_counts = {col: 0 for col in columns}\n",
    "\n",
    "    # If the frequency file exists for the current year\n",
    "    if os.path.exists(frequency_file):\n",
    "        with open(frequency_file, 'r', encoding='utf-8') as file:\n",
    "            # Read the frequency data into a dictionary\n",
    "            frequency_data = {}\n",
    "            for line in file:\n",
    "                word, count = line.strip().split()\n",
    "                frequency_data[word] = int(count)\n",
    "        \n",
    "        # Process each politician category\n",
    "        for col in columns:\n",
    "            # Get the list of names for the current politician category\n",
    "            names = politicians_df.loc[politicians_df['year'] == year, col].values[0]\n",
    "\n",
    "            # If the names are NaN, keep the count as NaN\n",
    "            if pd.isna(names):\n",
    "                year_counts[col] = None\n",
    "            else:\n",
    "                names = names.split('/')  # Handle multiple names separated by '/'\n",
    "                # Sum the counts for each name in the frequency data\n",
    "                for name in names:\n",
    "                    if name in frequency_data:\n",
    "                        year_counts[col] += frequency_data[name]\n",
    "    \n",
    "    # Append the results for the current year to the result dictionary\n",
    "    for col in columns:\n",
    "        result[col].append(year_counts[col])\n",
    "\n",
    "average_row = []\n",
    "\n",
    "for col in columns:\n",
    "    # Calculate the average, excluding NaN values\n",
    "    values = [val for val in result[col] if pd.notna(val)]\n",
    "    average = sum(values) / len(values) if values else 0\n",
    "    average_row.append(average)\n",
    "\n",
    "# Add the average row to the result\n",
    "result['year'].append('average')  # Add a label for the average row\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    result[col].append(average_row[i])\n",
    "\n",
    "# Convert the result dictionary to a DataFrame and save it as a CSV file\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.to_csv(output_file, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the frequency for \"韩国\" and \"北朝鲜\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "# Define the words to search for\n",
    "words_to_search = [\"韩国\", \"南朝鲜\"]\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "frequency_data = {word: [] for word in words_to_search}\n",
    "frequency_data['year'] = []\n",
    "\n",
    "# Path to the frequency folder\n",
    "frequency_folder = '../../corpus/frequency'\n",
    "\n",
    "# Iterate over each year file in the folder\n",
    "for year_file in sorted(os.listdir(frequency_folder)):\n",
    "    if year_file.endswith('.txt'):\n",
    "        year = int(year_file.split('.')[0])  # Extract the year from the file name\n",
    "        frequency_data['year'].append(year)\n",
    "        \n",
    "        # Initialize the counts for the current year\n",
    "        year_counts = {word: 0 for word in words_to_search}\n",
    "\n",
    "        # Open and read the file\n",
    "        with open(os.path.join(frequency_folder, year_file), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, count = line.strip().split()\n",
    "                count = int(count)\n",
    "                \n",
    "                # Update the counts if the word is one of the target words\n",
    "                if word in words_to_search:\n",
    "                    year_counts[word] += count\n",
    "\n",
    "        # Append the counts for the current year to the result dictionary\n",
    "        for word in words_to_search:\n",
    "            frequency_data[word].append(year_counts[word])\n",
    "\n",
    "# Convert the result dictionary to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(frequency_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "for word in words_to_search:\n",
    "    plt.plot(df['year'], df[word], label=word)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.savefig('south_korea.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the neighbourhood/path of South Korean presidents on 5-year slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "color_list = [\n",
    "    'limegreen', 'forestgreen', 'mediumseagreen', 'darkcyan',\n",
    "    'turquoise', 'deepskyblue', 'dodgerblue', 'royalblue', 'darkblue', 'darkslateblue',\n",
    "    'rebeccapurple', 'blueviolet', 'darkmagenta', 'orchid', 'plum'\n",
    "]\n",
    "\n",
    "\n",
    "def adjust_figsize(x, y, max_pixels=20):\n",
    "    width = max(x) - min(x)\n",
    "    height = max(y) - min(y)\n",
    "    current_pixels = max(width, height)\n",
    "    if current_pixels > max_pixels:\n",
    "        ratio = max_pixels / current_pixels\n",
    "        width = int(width * ratio)\n",
    "        height = int(height * ratio)\n",
    "    return width, height\n",
    "\n",
    "\n",
    "def neighbour_path(models, names, rs=49):\n",
    "    \"\"\"\n",
    "    Search for the nearest neighbours for target names at different times\n",
    "    Plot the path of name vectors across time in their neighbourhood\n",
    "    \"\"\"\n",
    "\n",
    "    csv_file = 'kr_neighbour.csv'\n",
    "    fieldnames = ['year', 'name'] + ['neighbour' + str(i + 1) for i in range(10)]\n",
    "\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "    labels = []\n",
    "    vectors = []\n",
    "    year = 1950\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        name = names[i]\n",
    "        labels.append(name + str(year))\n",
    "        vectors.append(model.wv[name])\n",
    "        neighbours = model.wv.most_similar(name, topn=10)\n",
    "\n",
    "        row = [year, name] + [pair[0] for pair in neighbours]\n",
    "        with open(csv_file, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "\n",
    "        for neighbour in neighbours[:5]:\n",
    "            # print(neighbour)\n",
    "            labels.append(neighbour[0])\n",
    "            vectors.append(model.wv[neighbour[0]])\n",
    "        year += 5\n",
    "\n",
    "    tsne_model = TSNE(perplexity=30, n_components=2, init='pca', n_iter=1000, random_state=rs)\n",
    "    flattend_2d = tsne_model.fit_transform(vectors)\n",
    "\n",
    "    x, y = [], []\n",
    "    for value in flattend_2d:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    x_line, y_line = [], []\n",
    "    for i in range(len(labels) // 6):\n",
    "        x_line.append(x[i * 6])\n",
    "        y_line.append(y[i * 6])\n",
    "\n",
    "    # print(max(x), min(x), max(y), min(y))\n",
    "    plt.figure(figsize=adjust_figsize(x, y))\n",
    "    plt.axis('off')\n",
    "    plt.plot(x_line, y_line, color='steelblue')\n",
    "\n",
    "    texts = []\n",
    "    for i in range(len(labels) // 6):\n",
    "        index = i * 6\n",
    "        color = color_list[i]\n",
    "        plt.scatter(x[index], y[index], s=40, c=color)\n",
    "        texts.append(plt.annotate(labels[index], xy=(x[index], y[index]),\n",
    "            fontproperties=FontProperties('Noto Sans SC', weight='bold'), fontsize = 25, color=color))\n",
    "        for j in range(5):\n",
    "            index += 1\n",
    "            plt.scatter(x[index], y[index], s=10, c=color)\n",
    "            texts.append(plt.annotate(labels[index], xy=(x[index], y[index]),\n",
    "                fontproperties=FontProperties('Noto Sans SC'), fontsize = 15, color=color))\n",
    "    \n",
    "    adjust_text(texts, iter_lim=1000)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kr_path.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "modeldir = '../../compass/5-year/model-cbow/'\n",
    "models_5 = []\n",
    "\n",
    "for i in range(1950, 2025, 5):\n",
    "    fiveyear = modeldir + str(i) + '-' + str(i + 4) + '.model'\n",
    "    model = Word2Vec.load(fiveyear)\n",
    "    models_5.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['李承晚', '李承晚', '朴正熙', '朴正熙', '朴正熙', '朴正熙', '全斗焕',\n",
    "         '全斗焕', '卢泰愚', '金泳三', '金大中', '卢武铉', '李明博', '朴槿惠', '文在寅']\n",
    "\n",
    "neighbour_path(models_5, names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
