{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "modeldir = '../../compass/5-year/model-cbow/'\n",
    "models = []\n",
    "\n",
    "for i in range(1945, 2025, 5):\n",
    "    fiveyear = modeldir + str(i) + '-' + str(i + 4) + '.model'\n",
    "    model = Word2Vec.load(fiveyear)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def cosine_dist(x,y):\n",
    "    num = np.dot(x, y)\n",
    "    denom = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "    return 1 - num / denom\n",
    "\n",
    "word_list = []\n",
    "with open('../../corpus/vocab_filter.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word_list.extend(line.split())\n",
    "\n",
    "results = []\n",
    "\n",
    "for word in word_list:\n",
    "    word_vectors = [model.wv[word] for model in models if word in model.wv]\n",
    "\n",
    "    # Compute cosine distances between adjacent models\n",
    "    distances = []\n",
    "    for i in range(len(word_vectors) - 1):\n",
    "        dist = cosine_dist(word_vectors[i], word_vectors[i + 1])\n",
    "        distances.append(dist)\n",
    "\n",
    "    # Calculate max and sum of distances\n",
    "    max_dist = max(distances)\n",
    "    sum_dist = sum(distances)\n",
    "\n",
    "    # Calculate cosine distance between first and last model\n",
    "    first_last_dist = cosine_dist(word_vectors[1], word_vectors[-2])\n",
    "\n",
    "    # Append results\n",
    "    results.append([word, max_dist, sum_dist, first_last_dist])\n",
    "\n",
    "csv_file = 'word_distances.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['word', 'max dist', 'sum dist', 'first-last dist'])\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically add code blocks to judgement.ipynb and draw.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load the notebook\n",
    "notebook_path = 'judgement.ipynb'\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Read the filtered words\n",
    "words_file_path = 'filtered_words.txt'\n",
    "with open(words_file_path, 'r', encoding='utf-8') as f:\n",
    "    words = [line.strip() for line in f]\n",
    "\n",
    "# Add code cells for each word\n",
    "for word in words:\n",
    "    # Create a new code cell\n",
    "    new_cell = nbformat.v4.new_code_cell(f'print_neighbourhood(\"{word}\")')\n",
    "    \n",
    "    # Append the new cell to the notebook\n",
    "    notebook.cells.append(new_cell)\n",
    "\n",
    "# Save the updated notebook\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(notebook, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load the notebook\n",
    "notebook_path = 'draw.ipynb'\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Read the filtered words\n",
    "words_file_path = 'filtered_words.txt'\n",
    "with open(words_file_path, 'r', encoding='utf-8') as f:\n",
    "    words = [line.strip() for line in f]\n",
    "\n",
    "# Add code cells for each word\n",
    "for word in words:\n",
    "    # Create a new code cell\n",
    "    new_cell = nbformat.v4.new_code_cell(f'neighbour_path(\"{word}\")')\n",
    "    \n",
    "    # Append the new cell to the notebook\n",
    "    notebook.cells.append(new_cell)\n",
    "\n",
    "# Save the updated notebook\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(notebook, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract entries from dictionary .mdx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readmdict import MDX, MDD\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = '../../corpus/dict.mdx'\n",
    "\n",
    "# Initialize the dictionary to store the key and list of values\n",
    "items_dict = defaultdict(list)\n",
    "\n",
    "# Iterate over each key-value pair in items\n",
    "for key, value in MDX(filename).items():\n",
    "    # Append the value to the list for each key\n",
    "    items_dict[key].append(value.decode())\n",
    "\n",
    "# Convert defaultdict back to a normal dictionary (optional)\n",
    "items = dict(items_dict)\n",
    "\n",
    "def get_entry(word):\n",
    "    key = word.encode()\n",
    "    if key not in items.keys():\n",
    "        return '-'\n",
    "    \n",
    "    cleaned_text = []\n",
    "\n",
    "    for text in items[key]:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "        # Find the main tag\n",
    "        main_content = soup.find('main')\n",
    "\n",
    "        # Remove the title and metadata sections\n",
    "        if main_content.find('span', class_='title'):\n",
    "            main_content.find('span', class_='title').decompose()\n",
    "        if main_content.find('div', class_='metadata'):\n",
    "            main_content.find('div', class_='metadata').decompose()\n",
    "\n",
    "        # Check if 'lya' exists\n",
    "        lya_element = main_content.find('span', class_='lya')\n",
    "        if lya_element:\n",
    "            lya_text = lya_element.get_text(strip=True) + ' '\n",
    "            lya_element.decompose()  # Remove 'lya' from the remaining content\n",
    "        else:\n",
    "            lya_text = ''\n",
    "\n",
    "        # Extract the remaining content and join them into a single string\n",
    "        cleaned_text.append(lya_text + ''.join(main_content.stripped_strings))\n",
    "\n",
    "    return ''.join(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the filtered words\n",
    "words_file_path = 'filtered_words.txt'\n",
    "dict_entry_path = 'dict_entry.txt'\n",
    "with open(words_file_path, 'r', encoding='utf-8') as f:\n",
    "    words = [line.strip() for line in f]\n",
    "\n",
    "with open(dict_entry_path, 'w', encoding='utf-8') as file:\n",
    "    for word in words:\n",
    "        entry = get_entry(word)\n",
    "        file.write(entry + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
